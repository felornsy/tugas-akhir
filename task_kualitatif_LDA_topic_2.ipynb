{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "xLTR8avada_P"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd #a library to make the data more structured55r56y6y\n",
        "\n",
        "# referensi\n",
        "# https://medium.com/@listari.tari/topic-modeling-menggunakan-latent-dirchlect-allocation-part-2-topic-modeling-with-gensim-c9ffd196cb87"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "SqJ6bJwOdmxH",
        "outputId": "4c42ea1b-af2c-4219-d627-0e55f2195cc1"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('data/text-preprocessing.csv')\n",
        "df = df.drop(columns=['Unnamed: 0'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxWR5yAqMP_S",
        "outputId": "d9a66715-3b7c-449c-9244-5372b4ced4b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(109705, 23)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "3PYA4slIdNBD"
      },
      "outputs": [],
      "source": [
        "df_new = df[df['tweet_tokens_stemmed_string'].notnull()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiih8RfJcG3C",
        "outputId": "cdec65c1-675f-4022-e661-e57038cd2a71"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    lomba poster ilmiah energi baru deadline janua...\n",
              "1                 elaahhh ngomong energi baru tv biruu\n",
              "2    daerah pencil membutuhkam listrik manfaat ener...\n",
              "3    tarik materi energi baru baek ganti nama propa...\n",
              "4    aneh inget jatropa alias minyak jarak bbrp jat...\n",
              "Name: tweet_tokens_stemmed_string, dtype: object"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_new['tweet_tokens_stemmed_string'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aox9mh-NFJaY",
        "outputId": "dc62a84f-cf29-41fc-a1b4-83dbeff37ddc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "109404\n"
          ]
        }
      ],
      "source": [
        "text = df_new['tweet_tokens_stemmed_string']\n",
        "text_list =  [i.split() for i in text]\n",
        "#text_list =  [i for i in text]\n",
        "print(len(text_list))\n",
        "# print(text_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4Y-nzFZdmOz",
        "outputId": "a63b5e1c-301a-4d94-f1bb-9d06647e3422"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frequency Tokens : \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0    [( , 11), (a, 10), (e, 6), (i, 6), (s, 5), (r,...\n",
              "1    [( , 5), (e, 3), (a, 3), (h, 3), (n, 3), (g, 3...\n",
              "2    [(a, 7), ( , 6), (e, 5), (r, 4), (i, 4), (m, 4...\n",
              "3    [(a, 10), ( , 8), (r, 5), (i, 5), (e, 4), (t, ...\n",
              "4    [(a, 12), ( , 12), (r, 8), (e, 7), (b, 6), (i,...\n",
              "Name: tweet_tokens_stemmed_string, dtype: object"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# NLTK calc frequency distribution\n",
        "from nltk.probability import FreqDist\n",
        "def freqDist_wrapper(text):\n",
        "    return FreqDist(text)\n",
        "\n",
        "df_new['tweet_tokens_stemmed_string'] = df_new['tweet_tokens_stemmed_string'].apply(freqDist_wrapper)\n",
        "\n",
        "print('Frequency Tokens : \\n') \n",
        "df_new['tweet_tokens_stemmed_string'].head().apply(lambda x : x.most_common())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "qnLvYqUHc5Jj"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "#Create Bigram & Trigram Models \n",
        "from gensim.models import Phrases\n",
        "# Add bigrams and trigrams to docs,minimum count 10 means only that appear 10 times or more.\n",
        "bigram = Phrases(text_list, min_count=10)\n",
        "trigram = Phrases(bigram[text_list])\n",
        "for idx in range(len(text_list)):\n",
        "    for token in bigram[text_list[idx]]:\n",
        "        if '_' in token:\n",
        "            # Token is a bigram, add to document.\n",
        "            text_list[idx].append(token)\n",
        "    for token in trigram[text_list[idx]]:\n",
        "        if '_' in token:\n",
        "            # Token is a bigram, add to document.\n",
        "            text_list[idx].append(token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EO5hqr3Ufk0S",
        "outputId": "0ff90687-35bf-4851-e1fd-5e3d61b8f9dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dictionary<16354 unique tokens: ['deadline', 'derajat', 'ilmiah', 'januari', 'lomba']...>\n"
          ]
        }
      ],
      "source": [
        "# modul gensim.corpora akan digunakan untuk membangun dictionary dari data teks sebelum dilakukan proses pembuatan model LDA.\n",
        "\n",
        "from gensim import corpora, models\n",
        "# Create a dictionary representation of the documents.\n",
        "dictionary = corpora.Dictionary(text_list)\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.2) \n",
        "#no_below (int, optional) – Keep tokens which are contained in at least no_below documents.\n",
        "#no_above (float, optional) – Keep tokens which are contained in no more than no_above documents (fraction of total corpus size, not an absolute number).\n",
        "print(dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oF92M3p_fqlQ",
        "outputId": "3b77b4e1-2901-4b52-9698-4ea50bb0fa14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "109404\n",
            "[(35, 1), (188, 1)]\n"
          ]
        }
      ],
      "source": [
        "#https://radimrehurek.com/gensim/tut1.html \n",
        "#build corpus\n",
        "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
        "doc_term_matrix = [dictionary.doc2bow(doc) for doc in text_list]\n",
        "#The function doc2bow converts document (a list of words) into the bag-of-words format\n",
        "'''The function doc2bow() simply counts the number of occurrences of each distinct word, \n",
        "converts the word to its integer word id and returns the result as a sparse vector. \n",
        "The sparse vector [(0, 1), (1, 1)] therefore reads: in the document “Human computer interaction”, \n",
        "the words computer (id 0) and human (id 1) appear once; \n",
        "the other ten dictionary words appear (implicitly) zero times.'''\n",
        "print(len(doc_term_matrix))\n",
        "print(doc_term_matrix[100])\n",
        "tfidf = models.TfidfModel(doc_term_matrix) #build TF-IDF model\n",
        "corpus_tfidf = tfidf[doc_term_matrix]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "m5ZUrlyFfx85"
      },
      "outputs": [],
      "source": [
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from numpy import array\n",
        "#function to compute coherence values\n",
        "def compute_coherence_values(dictionary, corpus, texts, limit, start, step):\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, iterations=100, random_state=1)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "        \n",
        "    return model_list, coherence_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "IBkB1bzOf3AP",
        "outputId": "73fe3950-434a-4fa4-8f87-77a708f4fe7c"
      },
      "outputs": [],
      "source": [
        "start=1\n",
        "limit=21\n",
        "step=1\n",
        "model_list, coherence_values = compute_coherence_values(dictionary, corpus=corpus_tfidf, \n",
        "                                                        texts=text_list, start=start, limit=limit, step=step)\n",
        "#show graphs\n",
        "import matplotlib.pyplot as plt\n",
        "x = range(start, limit, step)\n",
        "plt.plot(x, coherence_values)\n",
        "plt.xlabel(\"Num Topics\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherence_values\"), loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaNFv0PIgDYh",
        "outputId": "af7077fa-f1e8-4fb7-e0d6-d84eb319f207"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num Topics = 1  has Coherence Value of 0.517\n",
            "Num Topics = 2  has Coherence Value of 0.539\n",
            "Num Topics = 3  has Coherence Value of 0.436\n",
            "Num Topics = 4  has Coherence Value of 0.435\n",
            "Num Topics = 5  has Coherence Value of 0.423\n",
            "Num Topics = 6  has Coherence Value of 0.402\n",
            "Num Topics = 7  has Coherence Value of 0.424\n",
            "Num Topics = 8  has Coherence Value of 0.386\n",
            "Num Topics = 9  has Coherence Value of 0.41\n",
            "Num Topics = 10  has Coherence Value of 0.369\n",
            "Num Topics = 11  has Coherence Value of 0.332\n",
            "Num Topics = 12  has Coherence Value of 0.363\n",
            "Num Topics = 13  has Coherence Value of 0.346\n",
            "Num Topics = 14  has Coherence Value of 0.352\n",
            "Num Topics = 15  has Coherence Value of 0.321\n",
            "Num Topics = 16  has Coherence Value of 0.322\n",
            "Num Topics = 17  has Coherence Value of 0.321\n",
            "Num Topics = 18  has Coherence Value of 0.327\n",
            "Num Topics = 19  has Coherence Value of 0.328\n",
            "Num Topics = 20  has Coherence Value of 0.313\n"
          ]
        }
      ],
      "source": [
        "# Print the coherence scores\n",
        "for m, cv in zip(x, coherence_values):\n",
        "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWVlH8xAgPCu",
        "outputId": "7cc1b7d8-54f0-441e-cb60-e3aa1e8684fa"
      },
      "outputs": [],
      "source": [
        "model = LdaModel(corpus=corpus_tfidf, id2word=dictionary, num_topics=2, random_state=1) #num topic menyesuaikan hasil dari coherence value paling tinggi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topic: 1 Word: 0.013*\"pln\" + 0.013*\"pln_id\" + 0.009*\"kembang\" + 0.008*\"bangkit_listrik\" + 0.008*\"indonesia\" + 0.008*\"bangkit\" + 0.008*\"listrik\" + 0.006*\"menteri_esdm\" + 0.006*\"target\" + 0.006*\"capai_target\"\n",
            "Topic: 2 Word: 0.010*\"ruu\" + 0.008*\"erick_thohir\" + 0.005*\"menteri_bumn\" + 0.005*\"tinggi_indonesia\" + 0.005*\"larang_ekspor\" + 0.004*\"erick\" + 0.004*\"capai_persen\" + 0.004*\"kuat_bidang\" + 0.004*\"thohir\" + 0.004*\"mobil_listrik\"\n"
          ]
        }
      ],
      "source": [
        "for idx, topic in model.print_topics():\n",
        "    print('Topic: {} Word: {}'.format(idx+1, topic))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5CsIHbigduf",
        "outputId": "ad397ac7-6944-411e-f515-2a9344191f97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "top_words_per_topic = []\n",
        "for t in range(model.num_topics):\n",
        "    top_words_per_topic.extend([(t, ) + x for x in model.show_topic(t, topn = 5)]) #ubah2 yg ini\n",
        "# df=pd.DataFrame(top_words_per_topic, columns=['Topic', 'Word', 'P']).to_csv(\"top_words_topic_20.csv\")\n",
        "df = pd.DataFrame(top_words_per_topic, columns=['Topic', 'Word','P']).to_excel(\"data/lda_model_topic_2.xlsx\")\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSxbGusaEOl8",
        "outputId": "8776726e-83fd-480b-c4cb-9b3cdd8883a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyLDAvis in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (3.3.1)\n",
            "Requirement already satisfied: numexpr in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pyLDAvis) (2.8.3)\n",
            "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pyLDAvis) (1.4.3)\n",
            "Requirement already satisfied: future in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pyLDAvis) (0.18.2)\n",
            "Requirement already satisfied: scipy in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pyLDAvis) (1.9.0)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pyLDAvis) (3.1.2)\n",
            "Requirement already satisfied: funcy in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pyLDAvis) (1.17)\n",
            "Requirement already satisfied: setuptools in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pyLDAvis) (49.2.1)\n",
            "Requirement already satisfied: sklearn in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pyLDAvis) (0.0)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pyLDAvis) (1.1.1)\n",
            "Requirement already satisfied: gensim in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pyLDAvis) (4.2.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pyLDAvis) (1.23.1)\n",
            "Requirement already satisfied: joblib in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pyLDAvis) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2022.1)\n",
            "Requirement already satisfied: Cython==0.29.28 in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from gensim->pyLDAvis) (0.29.28)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from gensim->pyLDAvis) (6.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from jinja2->pyLDAvis) (2.1.1)\n",
            "Requirement already satisfied: packaging in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from numexpr->pyLDAvis) (21.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from scikit-learn->pyLDAvis) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.2.0->pyLDAvis) (1.16.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from packaging->numexpr->pyLDAvis) (3.0.9)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.2.1 -> 22.2.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "#ini dilakukan jika module belum tersedia\n",
        "!pip install pyLDAvis\n",
        "#!apt-get -qq install -y pyLDAvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BGTseD4gFRQ",
        "outputId": "39e9537e-6629-43ea-bfa0-e077c4ac7f89"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\asus\\OneDrive - Universitas Pertamina\\TA\\tugas-akhir\\venv\\lib\\site-packages\\past\\builtins\\misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
            "  from imp import reload\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PreparedData(topic_coordinates=             x    y  topics  cluster       Freq\n",
            "topic                                          \n",
            "0      0.20292  0.0       1        1  59.021431\n",
            "1     -0.20292  0.0       2        1  40.978569, topic_info=                  Term         Freq        Total Category  logprob  loglift\n",
            "7140            pln_id  2083.000000  2083.000000  Default  30.0000  30.0000\n",
            "150                pln  2100.000000  2100.000000  Default  29.0000  29.0000\n",
            "8163      erick_thohir   938.000000   938.000000  Default  28.0000  28.0000\n",
            "3362               ruu  1115.000000  1115.000000  Default  27.0000  27.0000\n",
            "783    bangkit_listrik  1345.000000  1345.000000  Default  26.0000  26.0000\n",
            "...                ...          ...          ...      ...      ...      ...\n",
            "16340  inggris_sepakat   311.871555   358.588895   Topic2  -5.8778   0.7525\n",
            "5179          sulawesi   294.874214   343.431656   Topic2  -5.9338   0.7397\n",
            "245                dpr   276.468615   318.719679   Topic2  -5.9983   0.7499\n",
            "16195      simak_bawah   328.173828   441.220499   Topic2  -5.8268   0.5961\n",
            "169              bahas   244.439940   358.317960   Topic2  -6.1214   0.5097\n",
            "\n",
            "[141 rows x 6 columns], token_table=       Topic      Freq                Term\n",
            "term                                      \n",
            "1323       1  0.016215           airlangga\n",
            "1323       2  0.985091           airlangga\n",
            "9086       1  0.010566  airlangga_hartarto\n",
            "9086       2  0.993209  airlangga_hartarto\n",
            "11837      2  1.000376                 ako\n",
            "...      ...       ...                 ...\n",
            "1049       1  0.035712                  uu\n",
            "1049       2  0.964225                  uu\n",
            "658        2  0.998148                 vii\n",
            "10521      1  0.073292          yantie_pln\n",
            "10521      2  0.926416          yantie_pln\n",
            "\n",
            "[216 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 2])\n"
          ]
        }
      ],
      "source": [
        "#import gensim\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models;pyLDAvis.enable_notebook()\n",
        "data = pyLDAvis.gensim_models.prepare(model, corpus_tfidf, dictionary)\n",
        "print(data)\n",
        "pyLDAvis.save_html(data, 'data/lda-gensim_clean_kualitatif2.html')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Data Kualitatif- LDA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "00a08332a9656d07fb15950983716ae1dc3baeb86e6935d123f31572ce96d1e2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
