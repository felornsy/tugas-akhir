{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xLTR8avada_P"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd #a library to make the data more structured55r56y6y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "SqJ6bJwOdmxH",
        "outputId": "4c42ea1b-af2c-4219-d627-0e55f2195cc1"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('data/text-preprocessing.csv')\n",
        "df = df.drop(columns=['Unnamed: 0'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3PYA4slIdNBD"
      },
      "outputs": [],
      "source": [
        "df_new = df[df['tweet_tokens_stemmed_string'].notnull()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiih8RfJcG3C",
        "outputId": "cdec65c1-675f-4022-e661-e57038cd2a71"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    lomba poster ilmiah energi baru deadline janua...\n",
              "1                 elaahhh ngomong energi baru tv biruu\n",
              "2    daerah pencil membutuhkam listrik manfaat ener...\n",
              "3    tarik materi energi baru baek ganti nama propa...\n",
              "4    aneh inget jatropa alias minyak jarak bbrp jat...\n",
              "Name: tweet_tokens_stemmed_string, dtype: object"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_new['tweet_tokens_stemmed_string'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aox9mh-NFJaY",
        "outputId": "dc62a84f-cf29-41fc-a1b4-83dbeff37ddc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "109404\n"
          ]
        }
      ],
      "source": [
        "text = df_new['tweet_tokens_stemmed_string']\n",
        "text_list =  [i.split() for i in text]\n",
        "#text_list =  [i for i in text]\n",
        "print(len(text_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4Y-nzFZdmOz",
        "outputId": "a63b5e1c-301a-4d94-f1bb-9d06647e3422"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frequency Tokens : \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0    [( , 11), (a, 10), (e, 6), (i, 6), (s, 5), (r,...\n",
              "1    [( , 5), (e, 3), (a, 3), (h, 3), (n, 3), (g, 3...\n",
              "2    [(a, 7), ( , 6), (e, 5), (r, 4), (i, 4), (m, 4...\n",
              "3    [(a, 10), ( , 8), (r, 5), (i, 5), (e, 4), (t, ...\n",
              "4    [(a, 12), ( , 12), (r, 8), (e, 7), (b, 6), (i,...\n",
              "Name: tweet_tokens_stemmed_string, dtype: object"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#masi salah hasil frekuensi nya\n",
        "# NLTK calc frequency distribution\n",
        "from nltk.probability import FreqDist\n",
        "def freqDist_wrapper(text):\n",
        "    return FreqDist(text)\n",
        "\n",
        "df_new['tweet_tokens_stemmed_string'] = df_new['tweet_tokens_stemmed_string'].apply(freqDist_wrapper)\n",
        "\n",
        "print('Frequency Tokens : \\n') \n",
        "df_new['tweet_tokens_stemmed_string'].head().apply(lambda x : x.most_common())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qnLvYqUHc5Jj"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "#Create Bigram & Trigram Models \n",
        "from gensim.models import Phrases\n",
        "# Add bigrams and trigrams to docs,minimum count 10 means only that appear 10 times or more.\n",
        "bigram = Phrases(text_list, min_count=10)\n",
        "trigram = Phrases(bigram[text_list])\n",
        "for idx in range(len(text_list)):\n",
        "    for token in bigram[text_list[idx]]:\n",
        "        if '_' in token:\n",
        "            # Token is a bigram, add to document.\n",
        "            text_list[idx].append(token)\n",
        "    for token in trigram[text_list[idx]]:\n",
        "        if '_' in token:\n",
        "            # Token is a bigram, add to document.\n",
        "            text_list[idx].append(token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EO5hqr3Ufk0S",
        "outputId": "0ff90687-35bf-4851-e1fd-5e3d61b8f9dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dictionary<16354 unique tokens: ['deadline', 'derajat', 'ilmiah', 'januari', 'lomba']...>\n"
          ]
        }
      ],
      "source": [
        "from gensim import corpora, models\n",
        "# Create a dictionary representation of the documents.\n",
        "dictionary = corpora.Dictionary(text_list)\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.2) \n",
        "#no_below (int, optional) – Keep tokens which are contained in at least no_below documents.\n",
        "#no_above (float, optional) – Keep tokens which are contained in no more than no_above documents (fraction of total corpus size, not an absolute number).\n",
        "print(dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oF92M3p_fqlQ",
        "outputId": "3b77b4e1-2901-4b52-9698-4ea50bb0fa14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "109404\n",
            "[(35, 1), (188, 1)]\n"
          ]
        }
      ],
      "source": [
        "#https://radimrehurek.com/gensim/tut1.html \n",
        "#build corpus\n",
        "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
        "doc_term_matrix = [dictionary.doc2bow(doc) for doc in text_list]\n",
        "#The function doc2bow converts document (a list of words) into the bag-of-words format\n",
        "'''The function doc2bow() simply counts the number of occurrences of each distinct word, \n",
        "converts the word to its integer word id and returns the result as a sparse vector. \n",
        "The sparse vector [(0, 1), (1, 1)] therefore reads: in the document “Human computer interaction”, \n",
        "the words computer (id 0) and human (id 1) appear once; \n",
        "the other ten dictionary words appear (implicitly) zero times.'''\n",
        "print(len(doc_term_matrix))\n",
        "print(doc_term_matrix[100])\n",
        "tfidf = models.TfidfModel(doc_term_matrix) #build TF-IDF model\n",
        "corpus_tfidf = tfidf[doc_term_matrix]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "m5ZUrlyFfx85"
      },
      "outputs": [],
      "source": [
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from numpy import array\n",
        "#function to compute coherence values\n",
        "def compute_coherence_values(dictionary, corpus, texts, limit, start, step):\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, iterations=100, random_state=1)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "        \n",
        "    return model_list, coherence_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "IBkB1bzOf3AP",
        "outputId": "73fe3950-434a-4fa4-8f87-77a708f4fe7c"
      },
      "outputs": [],
      "source": [
        "start=1\n",
        "limit=21\n",
        "step=1\n",
        "model_list, coherence_values = compute_coherence_values(dictionary, corpus=corpus_tfidf, \n",
        "                                                        texts=text_list, start=start, limit=limit, step=step)\n",
        "#show graphs\n",
        "import matplotlib.pyplot as plt\n",
        "x = range(start, limit, step)\n",
        "plt.plot(x, coherence_values)\n",
        "plt.xlabel(\"Num Topics\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherence_values\"), loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaNFv0PIgDYh",
        "outputId": "af7077fa-f1e8-4fb7-e0d6-d84eb319f207"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num Topics = 1  has Coherence Value of 0.517\n",
            "Num Topics = 2  has Coherence Value of 0.539\n",
            "Num Topics = 3  has Coherence Value of 0.436\n",
            "Num Topics = 4  has Coherence Value of 0.435\n",
            "Num Topics = 5  has Coherence Value of 0.423\n",
            "Num Topics = 6  has Coherence Value of 0.402\n",
            "Num Topics = 7  has Coherence Value of 0.424\n",
            "Num Topics = 8  has Coherence Value of 0.386\n",
            "Num Topics = 9  has Coherence Value of 0.41\n",
            "Num Topics = 10  has Coherence Value of 0.369\n",
            "Num Topics = 11  has Coherence Value of 0.332\n",
            "Num Topics = 12  has Coherence Value of 0.363\n",
            "Num Topics = 13  has Coherence Value of 0.346\n",
            "Num Topics = 14  has Coherence Value of 0.352\n",
            "Num Topics = 15  has Coherence Value of 0.321\n",
            "Num Topics = 16  has Coherence Value of 0.322\n",
            "Num Topics = 17  has Coherence Value of 0.321\n",
            "Num Topics = 18  has Coherence Value of 0.327\n",
            "Num Topics = 19  has Coherence Value of 0.328\n",
            "Num Topics = 20  has Coherence Value of 0.313\n"
          ]
        }
      ],
      "source": [
        "# Print the coherence scores\n",
        "for m, cv in zip(x, coherence_values):\n",
        "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWVlH8xAgPCu",
        "outputId": "7cc1b7d8-54f0-441e-cb60-e3aa1e8684fa"
      },
      "outputs": [],
      "source": [
        "model = LdaModel(corpus=corpus_tfidf, id2word=dictionary, num_topics=7, random_state=1) #num topic menyesuaikan hasil dari coherence value paling tinggi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topic: 1 Word: 0.030*\"bangkit_listrik\" + 0.021*\"capai_target\" + 0.020*\"bangkit\" + 0.018*\"target\" + 0.016*\"baur\" + 0.016*\"panas_bumi\" + 0.016*\"capai\" + 0.013*\"pln\" + 0.013*\"listrik\" + 0.012*\"kejar_target\"\n",
            "Topic: 2 Word: 0.052*\"target_baur\" + 0.043*\"menteri_bumn\" + 0.016*\"dana\" + 0.016*\"negara_maju\" + 0.015*\"tenaga_biogas\" + 0.014*\"alas\" + 0.014*\"untung\" + 0.013*\"anggar\" + 0.012*\"harga_jual\" + 0.011*\"subsidi\"\n",
            "Topic: 3 Word: 0.013*\"wujud\" + 0.009*\"rencana\" + 0.009*\"renewable_energy\" + 0.009*\"capai_persen\" + 0.009*\"milik_potensi\" + 0.008*\"cepat_transisi\" + 0.008*\"beli_listrik\" + 0.008*\"pln\" + 0.008*\"transisi\" + 0.008*\"indonesia_timur\"\n",
            "Topic: 4 Word: 0.022*\"erick_thohir\" + 0.012*\"kembang\" + 0.011*\"erick\" + 0.010*\"thohir\" + 0.009*\"investor\" + 0.009*\"panel_surya\" + 0.008*\"yantie_pln\" + 0.008*\"realisasi\" + 0.008*\"udien_pln\" + 0.007*\"tekan\"\n",
            "Topic: 5 Word: 0.033*\"menteri_esdm\" + 0.011*\"menteri\" + 0.011*\"esdm\" + 0.010*\"pltu\" + 0.009*\"tinggi_indonesia\" + 0.009*\"bahan_bakar\" + 0.008*\"buka_peluang\" + 0.008*\"kembang\" + 0.007*\"komisi_vii\" + 0.007*\"harga\"\n",
            "Topic: 6 Word: 0.046*\"pln_id\" + 0.023*\"pln\" + 0.018*\"ramah_lingkung\" + 0.018*\"id\" + 0.016*\"transisi\" + 0.010*\"ruu\" + 0.010*\"dukung\" + 0.010*\"indonesia\" + 0.010*\"plta\" + 0.009*\"kembang\"\n",
            "Topic: 7 Word: 0.015*\"plts_atap\" + 0.012*\"tenaga_surya\" + 0.011*\"operasi\" + 0.011*\"larang_ekspor\" + 0.010*\"tambah\" + 0.008*\"mobil_listrik\" + 0.008*\"jalan\" + 0.008*\"pres\" + 0.008*\"keren\" + 0.007*\"nuklir\"\n"
          ]
        }
      ],
      "source": [
        "for idx, topic in model.print_topics():\n",
        "    print('Topic: {} Word: {}'.format(idx+1, topic))\n",
        "    # yg belum remove Yang, yg,trus lower text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5CsIHbigduf",
        "outputId": "ad397ac7-6944-411e-f515-2a9344191f97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "top_words_per_topic = []\n",
        "for t in range(model.num_topics):\n",
        "    top_words_per_topic.extend([(t, ) + x for x in model.show_topic(t, topn = 5)]) #ubah2 yg ini\n",
        "# df=pd.DataFrame(top_words_per_topic, columns=['Topic', 'Word', 'P']).to_csv(\"top_words_topic_20.csv\")\n",
        "df = pd.DataFrame(top_words_per_topic, columns=['Topic', 'Word','P']).to_excel(\"data/lda_model_topic_7.xlsx\")\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSxbGusaEOl8",
        "outputId": "8776726e-83fd-480b-c4cb-9b3cdd8883a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyLDAvis in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (3.3.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pyLDAvis) (3.1.2)\n",
            "Requirement already satisfied: joblib in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pyLDAvis) (1.1.0)\n",
            "Requirement already satisfied: future in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pyLDAvis) (0.18.2)\n",
            "Requirement already satisfied: setuptools in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pyLDAvis) (49.2.1)\n",
            "Requirement already satisfied: scipy in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pyLDAvis) (1.9.0)\n",
            "Requirement already satisfied: numexpr in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pyLDAvis) (2.8.3)\n",
            "Requirement already satisfied: gensim in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pyLDAvis) (4.2.0)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pyLDAvis) (1.1.1)\n",
            "Requirement already satisfied: funcy in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pyLDAvis) (1.17)\n",
            "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pyLDAvis) (1.4.3)\n",
            "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pyLDAvis) (1.23.1)\n",
            "Requirement already satisfied: sklearn in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pyLDAvis) (0.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: Cython==0.29.28 in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from gensim->pyLDAvis) (0.29.28)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from gensim->pyLDAvis) (6.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from jinja2->pyLDAvis) (2.1.1)\n",
            "Requirement already satisfied: packaging in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from numexpr->pyLDAvis) (21.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from scikit-learn->pyLDAvis) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.2.0->pyLDAvis) (1.16.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from packaging->numexpr->pyLDAvis) (3.0.9)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.2.1 -> 22.2.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "#ini dilakukan jika module belum tersedia\n",
        "!pip install pyLDAvis\n",
        "#!apt-get -qq install -y pyLDAvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BGTseD4gFRQ",
        "outputId": "39e9537e-6629-43ea-bfa0-e077c4ac7f89"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\asus\\OneDrive - Universitas Pertamina\\TA\\tugas-akhir\\venv\\lib\\site-packages\\past\\builtins\\misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
            "  from imp import reload\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
            "topic                                                \n",
            "2     -0.035384 -0.238855       1        1  17.356396\n",
            "0      0.204753  0.095364       2        1  16.756378\n",
            "5      0.169429 -0.143969       3        1  16.706684\n",
            "4      0.066384  0.074683       4        1  14.383824\n",
            "6     -0.086128  0.246700       5        1  13.199615\n",
            "3      0.001788  0.005644       6        1  11.577854\n",
            "1     -0.320843 -0.039567       7        1  10.019249, topic_info=                  Term         Freq        Total Category  logprob  loglift\n",
            "7140            pln_id  2086.000000  2086.000000  Default  30.0000  30.0000\n",
            "1895       target_baur  1409.000000  1409.000000  Default  29.0000  29.0000\n",
            "3787      menteri_bumn  1165.000000  1165.000000  Default  28.0000  28.0000\n",
            "357       menteri_esdm  1310.000000  1310.000000  Default  27.0000  27.0000\n",
            "783    bangkit_listrik  1375.000000  1375.000000  Default  26.0000  26.0000\n",
            "...                ...          ...          ...      ...      ...      ...\n",
            "577              pulau   234.121457   254.845419   Topic7  -4.7560   2.2158\n",
            "198            subsidi   293.837633   429.316899   Topic7  -4.5288   1.9215\n",
            "999               jual   247.204461   334.444825   Topic7  -4.7016   1.9984\n",
            "314              hijau   208.154284   513.154393   Topic7  -4.8735   1.3984\n",
            "10994           bahlil   161.139213   212.498543   Topic7  -5.1296   2.0240\n",
            "\n",
            "[346 rows x 6 columns], token_table=       Topic      Freq        Term\n",
            "term                              \n",
            "1323       3  0.990037   airlangga\n",
            "128        7  0.995326         aku\n",
            "383        7  0.997524        alas\n",
            "389        3  0.323151        alih\n",
            "389        4  0.247259        alih\n",
            "...      ...       ...         ...\n",
            "10521      3  0.003772  yantie_pln\n",
            "10521      6  0.991973  yantie_pln\n",
            "2571       1  0.032322        zero\n",
            "2571       3  0.958894        zero\n",
            "2571       5  0.010774        zero\n",
            "\n",
            "[510 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 1, 6, 5, 7, 4, 2])\n"
          ]
        }
      ],
      "source": [
        "#import gensim\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models;pyLDAvis.enable_notebook()\n",
        "data = pyLDAvis.gensim_models.prepare(model, corpus_tfidf, dictionary)\n",
        "print(data)\n",
        "pyLDAvis.save_html(data, 'data/lda-gensim_clean_kualitatif7.html')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Data Kualitatif- LDA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "00a08332a9656d07fb15950983716ae1dc3baeb86e6935d123f31572ce96d1e2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
