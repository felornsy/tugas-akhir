{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xLTR8avada_P"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd #a library to make the data more structured55r56y6y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "SqJ6bJwOdmxH",
        "outputId": "4c42ea1b-af2c-4219-d627-0e55f2195cc1"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('data/text-preprocessing.csv')\n",
        "df = df.drop(columns=['Unnamed: 0'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3PYA4slIdNBD"
      },
      "outputs": [],
      "source": [
        "df_new = df[df['tweet_tokens_stemmed_string'].notnull()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uiih8RfJcG3C",
        "outputId": "cdec65c1-675f-4022-e661-e57038cd2a71"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    lomba poster ilmiah energi baru deadline janua...\n",
              "1                 elaahhh ngomong energi baru tv biruu\n",
              "2    daerah pencil membutuhkam listrik manfaat ener...\n",
              "3    tarik materi energi baru baek ganti nama propa...\n",
              "4    aneh inget jatropa alias minyak jarak bbrp jat...\n",
              "Name: tweet_tokens_stemmed_string, dtype: object"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_new['tweet_tokens_stemmed_string'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aox9mh-NFJaY",
        "outputId": "dc62a84f-cf29-41fc-a1b4-83dbeff37ddc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "109404\n"
          ]
        }
      ],
      "source": [
        "text = df_new['tweet_tokens_stemmed_string']\n",
        "text_list =  [i.split() for i in text]\n",
        "#text_list =  [i for i in text]\n",
        "print(len(text_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4Y-nzFZdmOz",
        "outputId": "a63b5e1c-301a-4d94-f1bb-9d06647e3422"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frequency Tokens : \n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0    [( , 11), (a, 10), (e, 6), (i, 6), (s, 5), (r,...\n",
              "1    [( , 5), (e, 3), (a, 3), (h, 3), (n, 3), (g, 3...\n",
              "2    [(a, 7), ( , 6), (e, 5), (r, 4), (i, 4), (m, 4...\n",
              "3    [(a, 10), ( , 8), (r, 5), (i, 5), (e, 4), (t, ...\n",
              "4    [(a, 12), ( , 12), (r, 8), (e, 7), (b, 6), (i,...\n",
              "Name: tweet_tokens_stemmed_string, dtype: object"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#masi salah hasil frekuensi nya\n",
        "# NLTK calc frequency distribution\n",
        "from nltk.probability import FreqDist\n",
        "def freqDist_wrapper(text):\n",
        "    return FreqDist(text)\n",
        "\n",
        "df_new['tweet_tokens_stemmed_string'] = df_new['tweet_tokens_stemmed_string'].apply(freqDist_wrapper)\n",
        "\n",
        "print('Frequency Tokens : \\n') \n",
        "df_new['tweet_tokens_stemmed_string'].head().apply(lambda x : x.most_common())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qnLvYqUHc5Jj"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "#Create Bigram & Trigram Models \n",
        "from gensim.models import Phrases\n",
        "# Add bigrams and trigrams to docs,minimum count 10 means only that appear 10 times or more.\n",
        "bigram = Phrases(text_list, min_count=10)\n",
        "trigram = Phrases(bigram[text_list])\n",
        "for idx in range(len(text_list)):\n",
        "    for token in bigram[text_list[idx]]:\n",
        "        if '_' in token:\n",
        "            # Token is a bigram, add to document.\n",
        "            text_list[idx].append(token)\n",
        "    for token in trigram[text_list[idx]]:\n",
        "        if '_' in token:\n",
        "            # Token is a bigram, add to document.\n",
        "            text_list[idx].append(token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EO5hqr3Ufk0S",
        "outputId": "0ff90687-35bf-4851-e1fd-5e3d61b8f9dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dictionary<16354 unique tokens: ['deadline', 'derajat', 'ilmiah', 'januari', 'lomba']...>\n"
          ]
        }
      ],
      "source": [
        "from gensim import corpora, models\n",
        "# Create a dictionary representation of the documents.\n",
        "dictionary = corpora.Dictionary(text_list)\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.2) \n",
        "#no_below (int, optional) – Keep tokens which are contained in at least no_below documents.\n",
        "#no_above (float, optional) – Keep tokens which are contained in no more than no_above documents (fraction of total corpus size, not an absolute number).\n",
        "print(dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oF92M3p_fqlQ",
        "outputId": "3b77b4e1-2901-4b52-9698-4ea50bb0fa14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "109404\n",
            "[(35, 1), (188, 1)]\n"
          ]
        }
      ],
      "source": [
        "#https://radimrehurek.com/gensim/tut1.html \n",
        "#build corpus\n",
        "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
        "doc_term_matrix = [dictionary.doc2bow(doc) for doc in text_list]\n",
        "#The function doc2bow converts document (a list of words) into the bag-of-words format\n",
        "'''The function doc2bow() simply counts the number of occurrences of each distinct word, \n",
        "converts the word to its integer word id and returns the result as a sparse vector. \n",
        "The sparse vector [(0, 1), (1, 1)] therefore reads: in the document “Human computer interaction”, \n",
        "the words computer (id 0) and human (id 1) appear once; \n",
        "the other ten dictionary words appear (implicitly) zero times.'''\n",
        "print(len(doc_term_matrix))\n",
        "print(doc_term_matrix[100])\n",
        "tfidf = models.TfidfModel(doc_term_matrix) #build TF-IDF model\n",
        "corpus_tfidf = tfidf[doc_term_matrix]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "m5ZUrlyFfx85"
      },
      "outputs": [],
      "source": [
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from numpy import array\n",
        "#function to compute coherence values\n",
        "def compute_coherence_values(dictionary, corpus, texts, limit, start, step):\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, iterations=100, random_state=1)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "        \n",
        "    return model_list, coherence_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "IBkB1bzOf3AP",
        "outputId": "73fe3950-434a-4fa4-8f87-77a708f4fe7c"
      },
      "outputs": [],
      "source": [
        "start=1\n",
        "limit=21\n",
        "step=1\n",
        "model_list, coherence_values = compute_coherence_values(dictionary, corpus=corpus_tfidf, \n",
        "                                                        texts=text_list, start=start, limit=limit, step=step)\n",
        "#show graphs\n",
        "import matplotlib.pyplot as plt\n",
        "x = range(start, limit, step)\n",
        "plt.plot(x, coherence_values)\n",
        "plt.xlabel(\"Num Topics\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherence_values\"), loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaNFv0PIgDYh",
        "outputId": "af7077fa-f1e8-4fb7-e0d6-d84eb319f207"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num Topics = 1  has Coherence Value of 0.517\n",
            "Num Topics = 2  has Coherence Value of 0.539\n",
            "Num Topics = 3  has Coherence Value of 0.436\n",
            "Num Topics = 4  has Coherence Value of 0.435\n",
            "Num Topics = 5  has Coherence Value of 0.423\n",
            "Num Topics = 6  has Coherence Value of 0.402\n",
            "Num Topics = 7  has Coherence Value of 0.424\n",
            "Num Topics = 8  has Coherence Value of 0.386\n",
            "Num Topics = 9  has Coherence Value of 0.41\n",
            "Num Topics = 10  has Coherence Value of 0.369\n",
            "Num Topics = 11  has Coherence Value of 0.332\n",
            "Num Topics = 12  has Coherence Value of 0.363\n",
            "Num Topics = 13  has Coherence Value of 0.346\n",
            "Num Topics = 14  has Coherence Value of 0.352\n",
            "Num Topics = 15  has Coherence Value of 0.321\n",
            "Num Topics = 16  has Coherence Value of 0.322\n",
            "Num Topics = 17  has Coherence Value of 0.321\n",
            "Num Topics = 18  has Coherence Value of 0.327\n",
            "Num Topics = 19  has Coherence Value of 0.328\n",
            "Num Topics = 20  has Coherence Value of 0.313\n"
          ]
        }
      ],
      "source": [
        "# Print the coherence scores\n",
        "for m, cv in zip(x, coherence_values):\n",
        "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWVlH8xAgPCu",
        "outputId": "7cc1b7d8-54f0-441e-cb60-e3aa1e8684fa"
      },
      "outputs": [],
      "source": [
        "model = LdaModel(corpus=corpus_tfidf, id2word=dictionary, num_topics=9, random_state=1) #num topic menyesuaikan hasil dari coherence value paling tinggi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topic: 1 Word: 0.067*\"pln_id\" + 0.039*\"bangkit_listrik\" + 0.035*\"pln\" + 0.030*\"capai_target\" + 0.023*\"panas_bumi\" + 0.022*\"bangkit\" + 0.014*\"sobat\" + 0.014*\"listrik\" + 0.012*\"target\" + 0.011*\"capai\"\n",
            "Topic: 2 Word: 0.033*\"ekspor\" + 0.030*\"larang_ekspor\" + 0.030*\"pasok_listrik\" + 0.027*\"simak_bawah\" + 0.023*\"simak\" + 0.022*\"mantap\" + 0.020*\"depan\" + 0.018*\"inggris\" + 0.018*\"pasok\" + 0.015*\"hambat\"\n",
            "Topic: 3 Word: 0.017*\"target_baur\" + 0.015*\"kapasitas\" + 0.011*\"rencana\" + 0.011*\"renewable_energy\" + 0.010*\"sumber_daya\" + 0.010*\"milik_potensi\" + 0.010*\"beli_listrik\" + 0.010*\"tambah_kapasitas\" + 0.009*\"indonesia_timur\" + 0.009*\"keren\"\n",
            "Topic: 4 Word: 0.024*\"bumn\" + 0.023*\"erick_thohir\" + 0.020*\"ruu\" + 0.017*\"komitmen\" + 0.015*\"kurang_emisi\" + 0.015*\"menteri_bumn\" + 0.012*\"emisi\" + 0.012*\"erick\" + 0.011*\"cepat_transisi\" + 0.010*\"thohir\"\n",
            "Topic: 5 Word: 0.044*\"menteri_esdm\" + 0.015*\"esdm\" + 0.015*\"menteri\" + 0.014*\"pltu\" + 0.013*\"tenaga_surya\" + 0.012*\"bahan_bakar\" + 0.010*\"buka_peluang\" + 0.010*\"tanah_air\" + 0.009*\"kembang\" + 0.009*\"dampak_positif\"\n",
            "Topic: 6 Word: 0.025*\"ramah_lingkung\" + 0.017*\"baur\" + 0.017*\"kejar_target\" + 0.015*\"plta_poso\" + 0.013*\"plta\" + 0.012*\"target\" + 0.011*\"plts_atap\" + 0.011*\"pln\" + 0.011*\"guna\" + 0.011*\"persen\"\n",
            "Topic: 7 Word: 0.047*\"tinggi_indonesia\" + 0.035*\"mobil_listrik\" + 0.031*\"panel_surya\" + 0.025*\"sistem\" + 0.020*\"tinggi\" + 0.017*\"selesai\" + 0.017*\"mobil\" + 0.016*\"anggar\" + 0.014*\"den\" + 0.014*\"surya\"\n",
            "Topic: 8 Word: 0.025*\"pertamina\" + 0.019*\"operasi\" + 0.013*\"rp_triliun\" + 0.013*\"rp\" + 0.012*\"bangun_infrastruktur\" + 0.012*\"tinggal\" + 0.012*\"kembang\" + 0.011*\"uu\" + 0.011*\"usaha\" + 0.011*\"nuklir\"\n",
            "Topic: 9 Word: 0.034*\"transisi\" + 0.029*\"id\" + 0.015*\"net_zero\" + 0.012*\"indonesia\" + 0.010*\"tambah\" + 0.010*\"hijau\" + 0.009*\"proyek\" + 0.009*\"biaya\" + 0.008*\"butuh\" + 0.008*\"kuat\"\n"
          ]
        }
      ],
      "source": [
        "for idx, topic in model.print_topics():\n",
        "    print('Topic: {} Word: {}'.format(idx+1, topic))\n",
        "    # yg belum remove Yang, yg,trus lower text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5CsIHbigduf",
        "outputId": "ad397ac7-6944-411e-f515-2a9344191f97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "top_words_per_topic = []\n",
        "for t in range(model.num_topics):\n",
        "    top_words_per_topic.extend([(t, ) + x for x in model.show_topic(t, topn = 5)]) #ubah2 yg ini\n",
        "# df=pd.DataFrame(top_words_per_topic, columns=['Topic', 'Word', 'P']).to_csv(\"top_words_topic_20.csv\")\n",
        "df = pd.DataFrame(top_words_per_topic, columns=['Topic', 'Word','P']).to_excel(\"data/lda_model_topic_9.xlsx\")\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSxbGusaEOl8",
        "outputId": "8776726e-83fd-480b-c4cb-9b3cdd8883a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyLDAvis in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (3.3.1)\n",
            "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pyLDAvis) (1.23.1)\n",
            "Requirement already satisfied: future in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pyLDAvis) (0.18.2)\n",
            "Requirement already satisfied: setuptools in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pyLDAvis) (49.2.1)\n",
            "Requirement already satisfied: numexpr in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pyLDAvis) (2.8.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pyLDAvis) (3.1.2)\n",
            "Requirement already satisfied: sklearn in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pyLDAvis) (0.0)\n",
            "Requirement already satisfied: scipy in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pyLDAvis) (1.9.0)\n",
            "Requirement already satisfied: gensim in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pyLDAvis) (4.2.0)\n",
            "Requirement already satisfied: funcy in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pyLDAvis) (1.17)\n",
            "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pyLDAvis) (1.4.3)\n",
            "Requirement already satisfied: joblib in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pyLDAvis) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pyLDAvis) (1.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2022.1)\n",
            "Requirement already satisfied: Cython==0.29.28 in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from gensim->pyLDAvis) (0.29.28)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from gensim->pyLDAvis) (6.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from jinja2->pyLDAvis) (2.1.1)\n",
            "Requirement already satisfied: packaging in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from numexpr->pyLDAvis) (21.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from scikit-learn->pyLDAvis) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.2.0->pyLDAvis) (1.16.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\asus\\onedrive - universitas pertamina\\ta\\tugas-akhir\\venv\\lib\\site-packages (from packaging->numexpr->pyLDAvis) (3.0.9)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.2.1 -> 22.2.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "#ini dilakukan jika module belum tersedia\n",
        "!pip install pyLDAvis\n",
        "#!apt-get -qq install -y pyLDAvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BGTseD4gFRQ",
        "outputId": "39e9537e-6629-43ea-bfa0-e077c4ac7f89"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\asus\\OneDrive - Universitas Pertamina\\TA\\tugas-akhir\\venv\\lib\\site-packages\\past\\builtins\\misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
            "  from imp import reload\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
            "topic                                                \n",
            "2      0.098420 -0.260032       1        1  17.218897\n",
            "5     -0.181101 -0.054815       2        1  13.099111\n",
            "0     -0.164824 -0.087987       3        1  12.239385\n",
            "8     -0.116885  0.055926       4        1  11.956173\n",
            "4     -0.092956  0.020339       5        1  11.551151\n",
            "3     -0.053884  0.239287       6        1  10.290377\n",
            "7     -0.011135 -0.015975       7        1   9.009042\n",
            "6      0.286286 -0.041247       8        1   7.853626\n",
            "1      0.236079  0.144504       9        1   6.782238, topic_info=                   Term         Freq        Total Category  logprob  loglift\n",
            "7140             pln_id  2236.000000  2236.000000  Default  30.0000  30.0000\n",
            "357        menteri_esdm  1370.000000  1370.000000  Default  29.0000  29.0000\n",
            "10440  tinggi_indonesia  1000.000000  1000.000000  Default  28.0000  28.0000\n",
            "783     bangkit_listrik  1445.000000  1445.000000  Default  27.0000  27.0000\n",
            "3661           transisi  1105.000000  1105.000000  Default  26.0000  26.0000\n",
            "...                 ...          ...          ...      ...      ...      ...\n",
            "1104               dana   213.559935   261.348593   Topic9  -4.4577   2.4889\n",
            "303                baca   132.374970   155.339589   Topic9  -4.9360   2.5309\n",
            "762              serius   180.836234   253.238043   Topic9  -4.6240   2.3541\n",
            "320               tekan   179.361155   277.470686   Topic9  -4.6322   2.2545\n",
            "5272             heroes   120.112927   169.225561   Topic9  -5.0332   2.3481\n",
            "\n",
            "[435 rows x 6 columns], token_table=       Topic      Freq            Term\n",
            "term                                  \n",
            "1323       2  0.992622       airlangga\n",
            "678        2  0.989173             akn\n",
            "3655       3  0.992711      akselerasi\n",
            "2836       9  0.988229    alir_listrik\n",
            "3574       1  0.992124            amat\n",
            "...      ...       ...             ...\n",
            "2013       6  0.560444           wujud\n",
            "6645       6  0.991376  wujud_komitmen\n",
            "10521      3  0.990009      yantie_pln\n",
            "10521      7  0.006149      yantie_pln\n",
            "2571       4  0.995181            zero\n",
            "\n",
            "[631 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 6, 1, 9, 5, 4, 8, 7, 2])\n"
          ]
        }
      ],
      "source": [
        "#import gensim\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models;pyLDAvis.enable_notebook()\n",
        "data = pyLDAvis.gensim_models.prepare(model, corpus_tfidf, dictionary)\n",
        "print(data)\n",
        "pyLDAvis.save_html(data, 'data/lda-gensim_clean_kualitatif9.html')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Data Kualitatif- LDA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "00a08332a9656d07fb15950983716ae1dc3baeb86e6935d123f31572ce96d1e2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
